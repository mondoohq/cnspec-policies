owner_mrn: //policy.api.mondoo.app
policies:
- uid: kubernetes-security
  name: Kubernetes Security Benchmark by Mondoo
  version: 1.0.2
  is_public: true
  authors:
    - name: Mondoo, Inc
      email: hello@mondoo.com
  docs:
    desc: |-
      The Kubernetes Workload Security Benchmark by Mondoo provides guidance for establishing secure Kubernetes workload deployments.
 
      If you have questions, comments, or have identified ways to improve this policy, please write us at hello@mondoo.io, or reach out in [GitHub Discussions](https://github.com/orgs/mondoohq/discussions).
  specs:
  - title: Kubernetes API Server
    asset_filter:
      query: |
        platform.family.contains(_ == 'linux')
        processes.where( executable == /kube-apiserver/ ).list != []
    scoring_queries:
      kubernetes-security-secure-kube-apiserver-yml:
      kubernetes-security-secure-etcd-data-dir:
      kubernetes-security-secure-admin-conf:
      kubernetes-security-secure-scheduler_conf:
      kubernetes-security-secure-controller-manager_conf:
      kubernetes-security-secure-pki-directory:
  - title: Kubernetes CronJobs
    asset_filter:
      query: platform.name == "k8s-cronjob"
    scoring_queries:
      kubernetes-security-cronjob-docker-socket:
      kubernetes-security-cronjob-containerd-socket:
      kubernetes-security-cronjob-crio-socket:
      kubernetes-security-cronjob-allowprivilegeescalation:
      kubernetes-security-cronjob-privilegedcontainer:
      kubernetes-security-cronjob-readonlyrootfilesystem:
      kubernetes-security-cronjob-runasnonroot:
      kubernetes-security-cronjob-hostnetwork:
      kubernetes-security-cronjob-hostpid:
      kubernetes-security-cronjob-hostipc:
      kubernetes-security-cronjob-serviceaccount:
      kubernetes-security-cronjob-imagepull:
      kubernetes-security-cronjob-limitcpu:
      kubernetes-security-cronjob-limitmemory:
      kubernetes-security-cronjob-capability-net-raw:
    data_queries:
  - title: Kubernetes StatefulSets
    asset_filter:
      query: platform.name == "k8s-statefulset"
    scoring_queries:
      kubernetes-security-statefulset-docker-socket:
      kubernetes-security-statefulset-containerd-socket:
      kubernetes-security-statefulset-crio-socket:
      kubernetes-security-statefulset-allowprivilegeescalation:
      kubernetes-security-statefulset-privilegedcontainer:
      kubernetes-security-statefulset-readonlyrootfilesystem:
      kubernetes-security-statefulset-runasnonroot:
      kubernetes-security-statefulset-hostnetwork:
      kubernetes-security-statefulset-hostpid:
      kubernetes-security-statefulset-hostipc:
      kubernetes-security-statefulset-serviceaccount:
      kubernetes-security-statefulset-imagepull:
      kubernetes-security-statefulset-limitcpu:
      kubernetes-security-statefulset-limitmemory:
      kubernetes-security-statefulset-capability-net-raw:
    data_queries:
  - title: Kubernetes Deployments
    asset_filter:
      query: platform.name == "k8s-deployment"
    scoring_queries:
      kubernetes-security-deployment-docker-socket:
      kubernetes-security-deployment-containerd-socket:
      kubernetes-security-deployment-crio-socket:
      kubernetes-security-deployment-allowprivilegeescalation:
      kubernetes-security-deployment-privilegedcontainer:
      kubernetes-security-deployment-readonlyrootfilesystem:
      kubernetes-security-deployment-runasnonroot:
      kubernetes-security-deployment-hostnetwork:
      kubernetes-security-deployment-hostpid:
      kubernetes-security-deployment-hostipc:
      kubernetes-security-deployment-serviceaccount:
      kubernetes-security-deployment-imagepull:
      kubernetes-security-deployment-limitcpu:
      kubernetes-security-deployment-limitmemory:
      kubernetes-security-deployment-capability-net-raw:
  - title: Kubernetes Jobs
    asset_filter:
      query: platform.name == "k8s-job"
    scoring_queries:
      kubernetes-security-job-docker-socket:
      kubernetes-security-job-containerd-socket:
      kubernetes-security-job-crio-socket:
      kubernetes-security-job-allowprivilegeescalation:
      kubernetes-security-job-privilegedcontainer:
      kubernetes-security-job-readonlyrootfilesystem:
      kubernetes-security-job-runasnonroot:
      kubernetes-security-job-hostnetwork:
      kubernetes-security-job-hostpid:
      kubernetes-security-job-hostipc:
      kubernetes-security-job-serviceaccount:
      kubernetes-security-job-imagepull:
      kubernetes-security-job-limitcpu:
      kubernetes-security-job-limitmemory:
      kubernetes-security-job-capability-net-raw:
    data_queries:
  - title: Kubernetes ReplicaSets
    asset_filter:
      query: platform.name == "k8s-replicaset"
    scoring_queries:
      kubernetes-security-replicaset-docker-socket:
      kubernetes-security-replicaset-containerd-socket:
      kubernetes-security-replicaset-crio-socket:
      kubernetes-security-replicaset-allowprivilegeescalation:
      kubernetes-security-replicaset-privilegedcontainer:
      kubernetes-security-replicaset-readonlyrootfilesystem:
      kubernetes-security-replicaset-runasnonroot:
      kubernetes-security-replicaset-hostnetwork:
      kubernetes-security-replicaset-hostpid:
      kubernetes-security-replicaset-hostipc:
      kubernetes-security-replicaset-serviceaccount:
      kubernetes-security-replicaset-imagepull:
      kubernetes-security-replicaset-limitcpu:
      kubernetes-security-replicaset-limitmemory:
      kubernetes-security-replicaset-capability-net-raw:
    data_queries:
  - title: Kubernetes DaemonSets
    asset_filter:
      query: platform.name == "k8s-daemonset"
    scoring_queries:
      kubernetes-security-daemonset-docker-socket:
      kubernetes-security-daemonset-containerd-socket:
      kubernetes-security-daemonset-crio-socket:
      kubernetes-security-daemonset-allowprivilegeescalation:
      kubernetes-security-daemonset-privilegedcontainer:
      kubernetes-security-daemonset-readonlyrootfilesystem:
      kubernetes-security-daemonset-runasnonroot:
      kubernetes-security-daemonset-hostnetwork:
      kubernetes-security-daemonset-hostpid:
      kubernetes-security-daemonset-hostipc:
      kubernetes-security-daemonset-serviceaccount:
      kubernetes-security-daemonset-imagepull:
      kubernetes-security-daemonset-limitcpu:
      kubernetes-security-daemonset-limitmemory:
      kubernetes-security-daemonset-capability-net-raw:
    data_queries:
  - title: Kubernetes Pods
    asset_filter:
      query: platform.name == "k8s-pod"
    scoring_queries:
      kubernetes-security-pod-docker-socket:
      kubernetes-security-pod-containerd-socket:
      kubernetes-security-pod-crio-socket:
      kubernetes-security-pod-allowprivilegeescalation:
      kubernetes-security-pod-privilegedcontainer:
      kubernetes-security-pod-readonlyrootfilesystem:
      kubernetes-security-pod-runasnonroot:
      kubernetes-security-pod-hostnetwork:
      kubernetes-security-pod-hostpid:
      kubernetes-security-pod-hostipc:
      kubernetes-security-pod-serviceaccount:
      kubernetes-security-pod-imagepull:
      kubernetes-security-pod-limitcpu:
      kubernetes-security-pod-limitmemory:
      kubernetes-security-pod-capability-net-raw:
    data_queries:
      # kubernetes-security-gather-pods-security-context:
      # kubernetes-security-gather-deployment-container:
      # kubernetes-security-gather-daemonset-container:
      # kubernetes-security-gather-statefulset-container:
      # kubernetes-security-gather-job-container:
      # kubernetes-security-gather-cronjob-container:
queries:
# api server / kubelet queries
- uid: kubernetes-security-secure-kube-apiserver-yml
  title: Set secure file permissions on the API server pod specification file
  docs:
    desc: Ensure that the API server pod specification file has permissions of `600` and is owned by `root:root`.
    remediation: |-
      Run this command on the Control Plane node:

      ```
      chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml
      chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml
      ```
  query: |
    if (file("/etc/kubernetes/manifests/kube-apiserver.yaml").exists) {
      file("/etc/kubernetes/manifests/kube-apiserver.yaml") {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == false
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "root"
        group.name == "root"
      }
    }
- uid: kubernetes-security-secure-etcd-data-dir
  title: Set secure directory permissions on the etcd data directory
  docs:
    desc: Ensure that the etcd data directory has permissions of `700` and is owned by `etcd:etcd`.
    remediation: |-
      On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:

      ```
      ps -ef | grep etcd
      ```

      Run the below command:

      ```
      chmod 700 /var/lib/etcd
      ```
  query: |
    if (file("/var/lib/etcd").exists) {
      file("/var/lib/etcd") {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == true
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "etcd"
        group.name == "etcd"
      }
    } else {
      dir = processes.where( executable == /etcd/ ).list[0].flags["data-dir"]
      file(dir) {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == true
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "etcd"
        group.name == "etcd"
      }
    }
- uid: kubernetes-security-secure-admin-conf
  title: Set secure file permissions on the admin.conf file
  docs:
    desc: Ensure that the `admin.conf` file has permissions of `600` and is owned by root:root.
    remediation: |-
      Run this command on the Control Plane node:

      ```
      chmod 600 /etc/kubernetes/admin.conf
      chown root:root /etc/kubernetes/admin.conf
      ```
  query: |
    if (file("/etc/kubernetes/admin.conf").exists) {
      file("/etc/kubernetes/admin.conf") {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == false
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "root"
        group.name == "root"
      }
    }
- uid: kubernetes-security-secure-scheduler_conf
  title: Set secure file permissions on the scheduler.conf file
  docs:
    desc: Ensure that the `scheduler.conf` file has permissions of `600` and is owned by `root:root`.
    remediation: |-
      Run this command on the Control Plane node:

      ```
      chmod 600 /etc/kubernetes/scheduler.conf
      chown root:root /etc/kubernetes/scheduler.conf
      ```
  query: |
    if (file("/etc/kubernetes/scheduler.conf").exists) {
      file("/etc/kubernetes/scheduler.conf") {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == false
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "root"
        group.name == "root"
      }
    }
- uid: kubernetes-security-secure-controller-manager_conf
  title: Set secure file permissions on the controller-manager.conf file
  docs:
    desc: Ensure that the `controller-manager.conf` file has permissions of `600` and is owned by `root:root`.
    remediation: |-
      Run this command on the Control Plane node:

      ```
      chmod 600 /etc/kubernetes/controller-manager.conf
      chown root:root /etc/kubernetes/controller-manager.conf
      ```
  query: |
    if (file("/etc/kubernetes/controller-manager.conf").exists) {
      file("/etc/kubernetes/controller-manager.conf") {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == false
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "root"
        group.name == "root"
      }
    }
- uid: kubernetes-security-secure-pki-directory
  title: Ensure that the Kubernetes PKI/SSL directory is owned by root:root
  docs:
    desc: Ensure that the Kubernetes PKI/SSL directory is owned by `root:root`.
    remediation: |-
      Run one of the following commands on the Control Plane node depending on the location of your PKI/SSL directory:

      ```
      chown -R root:root /etc/kubernetes/pki/
      ```
      
      or

      ```
      chown -R root:root /etc/kubernetes/ssl/
      ````
  query: |
    if (file("/etc/kubernetes/ssl/").exists) {
      file("/etc/kubernetes/ssl/") {
        user.name == "root"
        group.name == "root"
      }
    } else {
      file("/etc/kubernetes/pki/") {
        user.name == "root"
        group.name == "root"
      }
    }
# Pod queries
- uid: kubernetes-security-pod-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.pod {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: kubernetes-security-cronjob-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.cronjob {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: kubernetes-security-statefulset-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.statefulset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: kubernetes-security-deployment-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.deployment {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: kubernetes-security-job-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.job {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: kubernetes-security-replicaset-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.replicaset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: kubernetes-security-daemonset-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.daemonset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: kubernetes-security-pod-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.pod {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: kubernetes-security-cronjob-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.cronjob {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: kubernetes-security-statefulset-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.statefulset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: kubernetes-security-deployment-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.deployment {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: kubernetes-security-job-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.job {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: kubernetes-security-replicaset-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.replicaset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: kubernetes-security-daemonset-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.daemonset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: kubernetes-security-pod-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.pod {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: kubernetes-security-cronjob-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.cronjob {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: kubernetes-security-statefulset-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.statefulset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: kubernetes-security-deployment-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.deployment {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: kubernetes-security-job-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.job {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: kubernetes-security-replicaset-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.replicaset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: kubernetes-security-daemonset-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.daemonset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: kubernetes-security-pod-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: kubernetes-security-cronjob-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: kubernetes-security-statefulset-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: kubernetes-security-deployment-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: kubernetes-security-job-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: kubernetes-security-replicaset-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: kubernetes-security-daemonset-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: kubernetes-security-pod-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: kubernetes-security-cronjob-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: kubernetes-security-statefulset-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: kubernetes-security-deployment-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means that the container has the host's capabilities including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: kubernetes-security-job-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: kubernetes-security-replicaset-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: kubernetes-security-daemonset-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: kubernetes-security-pod-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: kubernetes-security-cronjob-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: kubernetes-security-statefulset-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: kubernetes-security-deployment-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: kubernetes-security-job-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: kubernetes-security-replicaset-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: kubernetes-security-daemonset-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: kubernetes-security-pod-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.pod {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: kubernetes-security-cronjob-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.cronjob {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: kubernetes-security-statefulset-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.statefulset {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: kubernetes-security-deployment-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.deployment {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: kubernetes-security-job-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.job {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: kubernetes-security-replicaset-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.replicaset {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: kubernetes-security-daemonset-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.daemonset {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: kubernetes-security-pod-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network including loopback devices. This capability can be used to intercept network traffic including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.pod {
      podSpec['hostNetwork'] != true
    }
- uid: kubernetes-security-cronjob-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.cronjob {
      podSpec['hostNetwork'] != true
    }
- uid: kubernetes-security-statefulset-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.statefulset {
      podSpec['hostNetwork'] != true
    }
- uid: kubernetes-security-deployment-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.deployment {
      podSpec['hostNetwork'] != true
    }
- uid: kubernetes-security-job-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.job {
      podSpec['hostNetwork'] != true
    }
- uid: kubernetes-security-replicaset-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.replicaset {
      podSpec['hostNetwork'] != true
    }
- uid: kubernetes-security-daemonset-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.daemonset {
      podSpec['hostNetwork'] != true
    }
- uid: kubernetes-security-pod-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.pod {
      podSpec['hostPID'] != true
    }
- uid: kubernetes-security-cronjob-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.cronjob {
      podSpec['hostPID'] != true
    }
- uid: kubernetes-security-statefulset-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.statefulset {
      podSpec['hostPID'] != true
    }
- uid: kubernetes-security-deployment-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.deployment {
      podSpec['hostPID'] != true
    }
- uid: kubernetes-security-job-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.job {
      podSpec['hostPID'] != true
    }
- uid: kubernetes-security-replicaset-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.replicaset {
      podSpec['hostPID'] != true
    }
- uid: kubernetes-security-daemonset-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.daemonset {
      podSpec['hostPID'] != true
    }
- uid: kubernetes-security-pod-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.pod {
      podSpec['hostIPC'] != true
    }
- uid: kubernetes-security-cronjob-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.cronjob {
      podSpec['hostIPC'] != true
    }
- uid: kubernetes-security-statefulset-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.statefulset {
      podSpec['hostIPC'] != true
    }
- uid: kubernetes-security-deployment-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.deployment {
      podSpec['hostIPC'] != true
    }
- uid: kubernetes-security-job-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.job {
      podSpec['hostIPC'] != true
    }
- uid: kubernetes-security-replicaset-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.replicaset {
      podSpec['hostIPC'] != true
    }
- uid: kubernetes-security-daemonset-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.daemonset {
      podSpec['hostIPC'] != true
    }
- uid: kubernetes-security-pod-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.pod {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: kubernetes-security-cronjob-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.cronjob {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: kubernetes-security-statefulset-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.statefulset {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: kubernetes-security-deployment-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.deployment {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: kubernetes-security-job-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.job {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: kubernetes-security-replicaset-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.replicaset {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: kubernetes-security-daemonset-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.daemonset {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: kubernetes-security-pod-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: kubernetes-security-cronjob-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: kubernetes-security-statefulset-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: kubernetes-security-deployment-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: kubernetes-security-job-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: kubernetes-security-replicaset-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: kubernetes-security-daemonset-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: kubernetes-security-pod-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: kubernetes-security-cronjob-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: kubernetes-security-statefulset-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: kubernetes-security-deployment-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: kubernetes-security-job-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: kubernetes-security-replicaset-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: kubernetes-security-daemonset-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: kubernetes-security-pod-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: kubernetes-security-cronjob-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: kubernetes-security-statefulset-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: kubernetes-security-deployment-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: kubernetes-security-job-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: kubernetes-security-replicaset-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: kubernetes-security-daemonset-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: kubernetes-security-pod-capability-net-raw
  title: Pods should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      Pods should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no Pods have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].securityContext.capabilities.add | . != null ) | select(.spec.containers[].securityContext.capabilities.add | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name'```


      Additionally, a Pod that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicity run with NET_RAW. List these Pods with:

      ```kubectl get pods -A -o json | jq -r '.items[] | select( (.spec.containers[].securityContext.capabilities.drop | . == null) or (.spec.containers[].securityContext.capabilities.drop | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name'```
    remediation: |
      For any Pods that explicitly add the NET_RAW or ALL capability, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: examplePod
        namespace: example-namespace
      spec:
        containers:
          - securityContext:
              capabilities:
                add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any Pods that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: example
        namespace: example-namespace
      spec:
        containers:
          - securityContext:
              capabilities:
                drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    # @msg Pod ${ _.name } should not run with NET_RAW capability
    k8s.pod {
      manifest['spec']['containers'] {
        _['securityContext']['capabilities'] {
          _['add'].none(_.upcase == "ALL")
          _['add'].none(_.upcase == "NET_RAW")
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: kubernetes-security-daemonset-capability-net-raw
  title: DaemonSets should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      DaemonSets should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no DaemonSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get daemonsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null ) | select(.spec.template.spec.containers[].securityContext.capabilities.add | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name'```


      Additionally, a DaemonSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicity run with NET_RAW. List these DaemonSets with:

      ```kubectl get daemonsets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].securityContext.capabilities.drop | . == null) or (.spec.template.spec.containers[].securityContext.capabilities.drop | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name'```
    remediation: |
      For any DaemonSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any DaemonSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    # @msg DaemonSet ${ _.name } should not run with NET_RAW capability
    k8s.daemonset {
      podSpec['containers'] {
        _['securityContext']['capabilitites'] {
          _['add'].none(_.upcase == "ALL")
          _['add'].none(_.upcase == "NET_RAW")
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: kubernetes-security-replicaset-capability-net-raw
  title: ReplicaSets should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      ReplicaSets should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no ReplicaSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get replicasets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null ) | select(.spec.template.spec.containers[].securityContext.capabilities.add | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name'```


      Additionally, a ReplicaSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicity run with NET_RAW. List these DaemonSets with:

      ```kubectl get replicasets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].securityContext.capabilities.drop | . == null) or (.spec.template.spec.containers[].securityContext.capabilities.drop | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name'```
    remediation: |
      For any ReplicaSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: ReplicaSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any ReplicaSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: apps/v1
      kind: ReplicaSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    # @msg ReplicaSet ${ _.name } should not run with NET_RAW capability
    k8s.replicaset {
      podSpec['containers'] {
        _['securityContext']['capabilitites'] {
          _['add'].none(_.upcase == "ALL")
          _['add'].none(_.upcase == "NET_RAW")
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: kubernetes-security-job-capability-net-raw
  title: Jobs should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      Jobs should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no Jobs have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get jobs -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null ) | select(.spec.template.spec.containers[].securityContext.capabilities.add | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name'```


      Additionally, a Job that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicity run with NET_RAW. List these DaemonSets with:

      ```kubectl get jobs -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].securityContext.capabilities.drop | . == null) or (.spec.template.spec.containers[].securityContext.capabilities.drop | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name'```
    remediation: |
      For any Jobs that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any Jobs that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    # @msg Job ${ _.name } should not run with NET_RAW capability
    k8s.job {
      podSpec['containers'] {
        _['securityContext']['capabilitites'] {
          _['add'].none(_.upcase == "ALL")
          _['add'].none(_.upcase == "NET_RAW")
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: kubernetes-security-deployment-capability-net-raw
  title: Deployments should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      Deployments should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no Jobs have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get deployments -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null ) | select(.spec.template.spec.containers[].securityContext.capabilities.add | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name'```


      Additionally, a Deployment that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicity run with NET_RAW. List these DaemonSets with:

      ```kubectl get deployments -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].securityContext.capabilities.drop | . == null) or (.spec.template.spec.containers[].securityContext.capabilities.drop | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name'```
    remediation: |
      For any Deployments that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any Deployments that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    # @msg Deployment ${ _.name } should not run with NET_RAW capability
    k8s.deployment {
      podSpec['containers'] {
        _['securityContext']['capabilitites'] {
          _['add'].none(_.upcase == "ALL")
          _['add'].none(_.upcase == "NET_RAW")
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: kubernetes-security-statefulset-capability-net-raw
  title: StatefulSets should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      StatefulSets should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no StatefulSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get statefulsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null ) | select(.spec.template.spec.containers[].securityContext.capabilities.add | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name'```


      Additionally, a StatefulSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicity run with NET_RAW. List these DaemonSets with:

      ```kubectl get statefulsets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].securityContext.capabilities.drop | . == null) or (.spec.template.spec.containers[].securityContext.capabilities.drop | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name'```
    remediation: |
      For any StatefulSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any StatefulSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    # @msg StatefulSet ${ _.name } should not run with NET_RAW capability
    k8s.statefulset {
      podSpec['containers'] {
        _['securityContext']['capabilitites'] {
          _['add'].none(_.upcase == "ALL")
          _['add'].none(_.upcase == "NET_RAW")
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: kubernetes-security-cronjob-capability-net-raw
  title: CronJobs should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      CronJobs should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no CronJobs have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get cronjobs -A -o json | jq -r '.items[] | select(.spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.add | . != null ) | select(.spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.add | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name'```


      Additionally, a CronJob that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicity run with NET_RAW. List these DaemonSets with:

      ```kubectl get cronjobs -A -o json | jq -r '.items[] | select( (.spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.drop | . == null) or (.spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.drop | any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name'```
    remediation: |
      For any CronJobs that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: batch/v1
      kind: CronJob
      metadata:
        name: example
        namespace: example-namespace
      spec:
        jobTemplate:
          spec:
            template:
              spec:
                containers:
                  - securityContext:
                      capabilities:
                        add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any CronJobs that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: batch/v1
      kind: CronJob
      metadata:
        name: example
        namespace: example-namespace
      spec:
        jobTemplate:
          spec:
            template:
              spec:
                containers:
                  - securityContext:
                      capabilities:
                        drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    # @msg CronJob ${ _.name } should not run with NET_RAW capability
    k8s.cronjob {
      podSpec['containers'] {
        _['securityContext']['capabilitites'] {
          _['add'].none(_.upcase == "ALL")
          _['add'].none(_.upcase == "NET_RAW")
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
# Data Queries
- uid: kubernetes-security-gather-deployment-container
  title: Gather all Deployments
  query: |
    k8s.deployments {
      name
    }
- uid: kubernetes-security-gather-daemonset-container
  title: Gather all DaemonSets
  query: |
    k8s.daemonsets {
      name
    }
- uid: kubernetes-security-gather-statefulset-container
  title: Gather all StatefulSets
  query: |
    k8s.statefulsets {
      name
    }
- uid: kubernetes-security-gather-job-container
  title: Gather all Jobs
  query: |
    k8s.jobs {
      name
    }
- uid: kubernetes-security-gather-cronjob-container
  title: Gather all CronJobs
  query: |
    k8s.cronjobs {
      name
    }
- uid: kubernetes-security-gather-pods-security-context
  title: Gather all Pods with securityContext
  query: |
    k8s.pods {
      name
      namespace
      initContainers {
        name
        resources
        securityContext
      }
      containers {
        name
        resources
        livenessProbe
        securityContext
      }
    }
