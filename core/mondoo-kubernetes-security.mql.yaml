owner_mrn: //policy.api.mondoo.app
policies:
- uid: mondoo-kubernetes-security
  name: Kubernetes Workload Security by Mondoo
  version: 1.0.2
  is_public: true
  authors:
    - name: Mondoo, Inc
      email: hello@mondoo.com
  docs:
    desc: |-
      The Kubernetes Workload Security by Mondoo provides guidance for establishing secure Kubernetes workload deployments.
 
      If you have questions, comments, or have identified ways to improve this policy, please write us at hello@mondoo.com, or reach out in [GitHub Discussions](https://github.com/orgs/mondoohq/discussions).
  props:
    allowedCiphers: ""
  specs:
  - title: Kubernetes API Server
    asset_filter:
      query: |
        platform.family.contains(_ == 'linux')
        processes.where( executable == /kube-apiserver/ ).list != []
    scoring_queries:
      mondoo-kubernetes-security-secure-kube-apiserver-yml:
      mondoo-kubernetes-security-secure-etcd-data-dir:
      mondoo-kubernetes-security-secure-admin-conf:
      mondoo-kubernetes-security-secure-scheduler_conf:
      mondoo-kubernetes-security-secure-controller-manager_conf:
      mondoo-kubernetes-security-secure-pki-directory:
      mondoo-kubernetes-security-https-api-server:
      mondoo-kubernetes-security-api-server-no-anonymous-auth:
  - title: Kubernetes kubelet
    asset_filter:
      query: |
        platform.family.contains(_ == 'linux')
        processes.where( executable == /kubelet/ ).list != []
    scoring_queries:
      mondoo-kubernetes-security-kubelet-anonymous-authentication:
      mondoo-kubernetes-security-kubelet-event-record-qps:
      mondoo-kubernetes-security-kubelet-iptables-util-chains:
      mondoo-kubernetes-security-kubelet-protect-kernel-defaults:
      mondoo-kubernetes-security-kubelet-read-only-port:
      mondoo-kubernetes-security-kubelet-authorization-mode:
      mondoo-kubernetes-security-kubelet-strong-ciphers:
      mondoo-kubernetes-security-kubelet-tls-certificate:
      mondoo-kubernetes-security-kubelet-rotate-certificates:
      mondoo-kubernetes-security-secure-kubelet-config:
      mondoo-kubernetes-security-secure-kubelet-cert-authorities:
  - title: Kubernetes CronJobs Security by Mondoo
    asset_filter:
      query: platform.name == "k8s-cronjob"
    scoring_queries:
      mondoo-kubernetes-security-cronjob-docker-socket:
      mondoo-kubernetes-security-cronjob-containerd-socket:
      mondoo-kubernetes-security-cronjob-crio-socket:
      mondoo-kubernetes-security-cronjob-allowprivilegeescalation:
      mondoo-kubernetes-security-cronjob-privilegedcontainer:
      mondoo-kubernetes-security-cronjob-readonlyrootfilesystem:
      mondoo-kubernetes-security-cronjob-runasnonroot:
      mondoo-kubernetes-security-cronjob-hostnetwork:
      mondoo-kubernetes-security-cronjob-hostpid:
      mondoo-kubernetes-security-cronjob-hostipc:
      mondoo-kubernetes-security-cronjob-serviceaccount:
      mondoo-kubernetes-security-cronjob-imagepull:
      mondoo-kubernetes-security-cronjob-limitcpu:
      mondoo-kubernetes-security-cronjob-limitmemory:
      mondoo-kubernetes-security-cronjob-capability-net-raw:
      mondoo-kubernetes-security-cronjob-capability-sys-admin:
      mondoo-kubernetes-security-cronjob-ports-hostport:
      mondoo-kubernetes-security-cronjob-hostpath-readonly:
    data_queries:
  - title: Kubernetes StatefulSets Security by Mondoo
    asset_filter:
      query: platform.name == "k8s-statefulset"
    scoring_queries:
      mondoo-kubernetes-security-statefulset-docker-socket:
      mondoo-kubernetes-security-statefulset-containerd-socket:
      mondoo-kubernetes-security-statefulset-crio-socket:
      mondoo-kubernetes-security-statefulset-allowprivilegeescalation:
      mondoo-kubernetes-security-statefulset-privilegedcontainer:
      mondoo-kubernetes-security-statefulset-readonlyrootfilesystem:
      mondoo-kubernetes-security-statefulset-runasnonroot:
      mondoo-kubernetes-security-statefulset-hostnetwork:
      mondoo-kubernetes-security-statefulset-hostpid:
      mondoo-kubernetes-security-statefulset-hostipc:
      mondoo-kubernetes-security-statefulset-serviceaccount:
      mondoo-kubernetes-security-statefulset-imagepull:
      mondoo-kubernetes-security-statefulset-limitcpu:
      mondoo-kubernetes-security-statefulset-limitmemory:
      mondoo-kubernetes-security-statefulset-capability-net-raw:
      mondoo-kubernetes-security-statefulset-capability-sys-admin:
      mondoo-kubernetes-security-statefulset-ports-hostport:
      mondoo-kubernetes-security-statefulset-hostpath-readonly:
    data_queries:
  - title: Kubernetes Deployments Security by Mondoo
    asset_filter:
      query: platform.name == "k8s-deployment"
    scoring_queries:
      mondoo-kubernetes-security-deployment-docker-socket:
      mondoo-kubernetes-security-deployment-containerd-socket:
      mondoo-kubernetes-security-deployment-crio-socket:
      mondoo-kubernetes-security-deployment-allowprivilegeescalation:
      mondoo-kubernetes-security-deployment-privilegedcontainer:
      mondoo-kubernetes-security-deployment-readonlyrootfilesystem:
      mondoo-kubernetes-security-deployment-runasnonroot:
      mondoo-kubernetes-security-deployment-hostnetwork:
      mondoo-kubernetes-security-deployment-hostpid:
      mondoo-kubernetes-security-deployment-hostipc:
      mondoo-kubernetes-security-deployment-serviceaccount:
      mondoo-kubernetes-security-deployment-imagepull:
      mondoo-kubernetes-security-deployment-limitcpu:
      mondoo-kubernetes-security-deployment-limitmemory:
      mondoo-kubernetes-security-deployment-capability-net-raw:
      mondoo-kubernetes-security-deployment-capability-sys-admin:
      mondoo-kubernetes-security-deployment-ports-hostport:
      mondoo-kubernetes-security-deployment-hostpath-readonly:
      mondoo-kubernetes-security-deployment-tiller:
      mondoo-kubernetes-security-deployment-k8s-dashboard:
  - title: Kubernetes Jobs Security by Mondoo
    asset_filter:
      query: platform.name == "k8s-job"
    scoring_queries:
      mondoo-kubernetes-security-job-docker-socket:
      mondoo-kubernetes-security-job-containerd-socket:
      mondoo-kubernetes-security-job-crio-socket:
      mondoo-kubernetes-security-job-allowprivilegeescalation:
      mondoo-kubernetes-security-job-privilegedcontainer:
      mondoo-kubernetes-security-job-readonlyrootfilesystem:
      mondoo-kubernetes-security-job-runasnonroot:
      mondoo-kubernetes-security-job-hostnetwork:
      mondoo-kubernetes-security-job-hostpid:
      mondoo-kubernetes-security-job-hostipc:
      mondoo-kubernetes-security-job-serviceaccount:
      mondoo-kubernetes-security-job-imagepull:
      mondoo-kubernetes-security-job-limitcpu:
      mondoo-kubernetes-security-job-limitmemory:
      mondoo-kubernetes-security-job-capability-net-raw:
      mondoo-kubernetes-security-job-capability-sys-admin:
      mondoo-kubernetes-security-job-ports-hostport:
      mondoo-kubernetes-security-job-hostpath-readonly:
    data_queries:
  - title: Kubernetes ReplicaSets Security by Mondoo
    asset_filter:
      query: platform.name == "k8s-replicaset"
    scoring_queries:
      mondoo-kubernetes-security-replicaset-docker-socket:
      mondoo-kubernetes-security-replicaset-containerd-socket:
      mondoo-kubernetes-security-replicaset-crio-socket:
      mondoo-kubernetes-security-replicaset-allowprivilegeescalation:
      mondoo-kubernetes-security-replicaset-privilegedcontainer:
      mondoo-kubernetes-security-replicaset-readonlyrootfilesystem:
      mondoo-kubernetes-security-replicaset-runasnonroot:
      mondoo-kubernetes-security-replicaset-hostnetwork:
      mondoo-kubernetes-security-replicaset-hostpid:
      mondoo-kubernetes-security-replicaset-hostipc:
      mondoo-kubernetes-security-replicaset-serviceaccount:
      mondoo-kubernetes-security-replicaset-imagepull:
      mondoo-kubernetes-security-replicaset-limitcpu:
      mondoo-kubernetes-security-replicaset-limitmemory:
      mondoo-kubernetes-security-replicaset-capability-net-raw:
      mondoo-kubernetes-security-replicaset-capability-sys-admin:
      mondoo-kubernetes-security-replicaset-ports-hostport:
      mondoo-kubernetes-security-replicaset-hostpath-readonly:
    data_queries:
  - title: Kubernetes DaemonSets Security by Mondoo
    asset_filter:
      query: platform.name == "k8s-daemonset"
    scoring_queries:
      mondoo-kubernetes-security-daemonset-docker-socket:
      mondoo-kubernetes-security-daemonset-containerd-socket:
      mondoo-kubernetes-security-daemonset-crio-socket:
      mondoo-kubernetes-security-daemonset-allowprivilegeescalation:
      mondoo-kubernetes-security-daemonset-privilegedcontainer:
      mondoo-kubernetes-security-daemonset-readonlyrootfilesystem:
      mondoo-kubernetes-security-daemonset-runasnonroot:
      mondoo-kubernetes-security-daemonset-hostnetwork:
      mondoo-kubernetes-security-daemonset-hostpid:
      mondoo-kubernetes-security-daemonset-hostipc:
      mondoo-kubernetes-security-daemonset-serviceaccount:
      mondoo-kubernetes-security-daemonset-imagepull:
      mondoo-kubernetes-security-daemonset-limitcpu:
      mondoo-kubernetes-security-daemonset-limitmemory:
      mondoo-kubernetes-security-daemonset-capability-net-raw:
      mondoo-kubernetes-security-daemonset-capability-sys-admin:
      mondoo-kubernetes-security-daemonset-ports-hostport:
      mondoo-kubernetes-security-daemonset-hostpath-readonly:
    data_queries:
  - title: Kubernetes Pods Security by Mondoo
    asset_filter:
      query: platform.name == "k8s-pod"
    scoring_queries:
      mondoo-kubernetes-security-pod-docker-socket:
      mondoo-kubernetes-security-pod-containerd-socket:
      mondoo-kubernetes-security-pod-crio-socket:
      mondoo-kubernetes-security-pod-allowprivilegeescalation:
      mondoo-kubernetes-security-pod-privilegedcontainer:
      mondoo-kubernetes-security-pod-readonlyrootfilesystem:
      mondoo-kubernetes-security-pod-runasnonroot:
      mondoo-kubernetes-security-pod-hostnetwork:
      mondoo-kubernetes-security-pod-hostpid:
      mondoo-kubernetes-security-pod-hostipc:
      mondoo-kubernetes-security-pod-serviceaccount:
      mondoo-kubernetes-security-pod-imagepull:
      mondoo-kubernetes-security-pod-limitcpu:
      mondoo-kubernetes-security-pod-limitmemory:
      mondoo-kubernetes-security-pod-capability-net-raw:
      mondoo-kubernetes-security-pod-capability-sys-admin:
      mondoo-kubernetes-security-pod-ports-hostport:
      mondoo-kubernetes-security-pod-hostpath-readonly:
      mondoo-kubernetes-security-pod-tiller:
      mondoo-kubernetes-security-pod-k8s-dashboard:
    data_queries:
      # mondoo-kubernetes-security-gather-pods-security-context:
      # mondoo-kubernetes-security-gather-deployment-container:
      # mondoo-kubernetes-security-gather-daemonset-container:
      # mondoo-kubernetes-security-gather-statefulset-container:
      # mondoo-kubernetes-security-gather-job-container:
      # mondoo-kubernetes-security-gather-cronjob-container:
props:
  - uid: allowedCiphers
    title: Define the hardened SSL/ TLS ciphers
    query: |
      return ["TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256",
        "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA",
        "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384", "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
        "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA",
        "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
        "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
        "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305", "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256",
        "TLS_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_GCM_SHA256",
        "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_GCM_SHA384"]
queries:
# api server / kubelet queries
- uid: mondoo-kubernetes-security-kubelet-anonymous-authentication
  title: Disable anonymous authentication for kubelet
  severity: 100
  docs:
    desc: |
      Ensure that the kubelet is configured to disable anonymous requests to the kubelet server.
    audit: |
      If running the kubelet with the CLI parameter '--anonymous-auth', or running with 'authentication.anonymous.enabled' defined in the kubelet configuration file, ensure that the value is set to 'false'.
    remediation: |
      Set the '--anonymous-auth' CLI parameter and/or the 'authentication.anonymous.enabled' field in the kubelet configuration file to 'false'.
  query: |
    k8s.kubelet.configuration['authentication']['anonymous']['enabled'] == false
- uid: mondoo-kubernetes-security-kubelet-event-record-qps
  title: Configure kubelet to capture all event creation
  severity: 30
  docs:
    desc: |
      Ensure that the kubelet is configured to capture all event creation so as to avoid potentially not logging important events.
    audit: |
      If running the kubelet with the CLI parameter '--event-qps', or running with 'eventRecordQPS' defined in the kubelet configuration file, ensure that the value is set to '0'.
    remediation: |
      Set the '--event-qps' CLI parameter and/or the 'eventRecordQPS' field in the kubelet configuration file to '0'.
  query: |
    k8s.kubelet.configuration['eventRecordQPS'] == 0
- uid: mondoo-kubernetes-security-kubelet-iptables-util-chains
  title: Configure kubelet to ensure IPTables rules are set on host
  severity: 30
  docs:
    desc: |
      Ensure that the kubelet is set up to create IPTable utility rules for various kubernetes components.
    audit: |
      If running the kubelet with the CLI parameter '--make-iptables-util-chains', or running with 'makeIPTablesUtilChains' defined in the kubelet configuration file, ensure that the value is set to 'true'.
    remediation: |
      Set the '--make-iptables-util-chains' CLI parameter and/or the 'makeIPTablesUtilChains' field in the kubelet configuration file to 'true'.
  query: |
    k8s.kubelet.configuration['makeIPTablesUtilChains'] == true
- uid: mondoo-kubernetes-security-kubelet-protect-kernel-defaults
  title: Configure kubelet to protect kernel defaults
  severity: 60
  docs:
    desc: |
      Ensure that the kubelet is set up to error if the underlying kernel tunables are different that the kubelet defaults. By default the kubelet will attempt to modify the kernel as the kubelet starts up.
    audit: |
      If running the kubelet with the CLI parameter '--protect-kernel-defaults', or running with 'protectKernelDefaults' defined in the kubelet configuration file, ensure that the value is set to 'true'.
    remediation: |
      Set the '--protect-kernel-defaults' CLI parameter and/or the 'readOnlyPort' field in the kubelet configuration file to 'true'.
  query: |
    k8s.kubelet.configuration["protectKernelDefaults"] == "true"
- uid: mondoo-kubernetes-security-kubelet-read-only-port
  title: Do not allow unauthenticated read-only port on kubelet
  severity: 60
  docs:
    desc: |
      Ensure the kubelet is not configured to serve up unauthenticated read-only access.
    audit: |
      If running the kubelet with the CLI parameter '--read-only-port', or running with 'readOnlyPort' defined in the kubelet configuration file, ensure that the value is either '0' or simply not set ('0' is the default).
    remediation: |
      Set the '--read-only-port' CLI parameter or the 'readOnlyPort' field in the kubelet configuration file to '0'.
  query: |
    k8s.kubelet.configuration['readOnlyPort'] == 0 || k8s.kubelet.configuration['readOnlyPort'] == null
- uid: mondoo-kubernetes-security-kubelet-authorization-mode
  title: Ensure the kubelet is not configured with the AlwaysAllow authorization mode.
  severity: 100
  docs:
    desc: |
      Ensure the kubelet is not configured with the AlwaysAllow authorization mode.
    audit: |
      If running the kubelet with the CLI parameter '--authorization-mode', or running with 'authorization.mode' defined in the kubelet configuration file, ensure that the value is not set to 'AlwaysAllow'.
    remediation: |
      If the kubelet is configured with the CLI parameter '--authorization-mode', set it to something that isn't 'AlwaysAllow' (eg 'Webhook').

      If the kubelet is configured via the kubelet config file with the 'authorization.mode' parameter, set it to something that isn't 'AlwaysAllow' (eg. 'Webhook').
  query: |
    k8s.kubelet.configuration['authorization']['mode'] != "AlwaysAllow"
- uid: mondoo-kubernetes-security-kubelet-strong-ciphers
  title: Configure kubelet to use only strong cryptography
  severity: 100
  docs:
    desc: |
      Ensure the kubelet runs with only strong cryptography support.
    audit: |
      If running the kubelet with the CLI parameter '--tls-cipher-suites', or running with 'tlsCipherSuites' defined in the kubelet configuration file, ensure that the list of allowed ciphers is not empty and that all included ciphers are included in the following list:

      "TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256",
      "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384", "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
      "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA",
      "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256", "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
      "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305", "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_RSA_WITH_3DES_EDE_CBC_SHA",
      "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_GCM_SHA256", "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_GCM_SHA384"
    remediation: |
      Define the list of allowed TLS ciphers to include only items from the strong list of ciphers.

      If the kubelet is configured with the CLI parameter '--tls-cipher-suites', update the list (or define the parameter) to only include strong ciphers.

      If the kubelet is configured via the kubelet config file with the 'tlsCipherSuites' parameter, update the list (or create an entry for 'tlsCipherSuites') to only include string ciphers.
  query: |
    k8s.kubelet.configuration['tlsCipherSuites'] != null
    if (k8s.kubelet.configuration['tlsCipherSuites'] != null) {
      k8s.kubelet.configuration['tlsCipherSuites'].map( _.trim ).containsOnly(props.allowedCiphers)
    }
- uid: mondoo-kubernetes-security-kubelet-tls-certificate
  title: Run kubelet with a user-provided certificate/key
  severity: 100
  docs:
    desc: |
      Ensure that the kubelet is not running with self-signed certificates generated by the kubelet itself.
    audit: |
      The kubelet CLI parameters override values in the kubelet configuration file.

      Check the kubelet CLI parameters to see whether '--tls-cert-file' and '--tls-private-key' are set to a non-empty path/string.

      Check the kubelet configuration file to see whether 'tlsCertFile' and 'tlsPrivateKeyFile' are set to a non-empty path/string.
    remediation: |
      Configure the kubelet to use a user-provided certificate/key pair for serving up HTTPS.

      After acquiring the TLS certificate/key pair, update the kubelet configuration file

      Or if using the deprecated kubelet CLI parameters, update the '--tls-cert-file' and '--tls-private-key-file' parameters to use the new certificate/key.
  query: |
    k8s.kubelet.configuration["tlsCertFile"] != null
    k8s.kubelet.configuration["tlsPrivateKeyFile"] != null
- uid: mondoo-kubernetes-security-kubelet-rotate-certificates
  title: Run kubelet with automatic certificate rotation
  severity: 80
  docs:
    desc: |
      Ensure the kubelet is running with automatic certificate rotation so that the kubelet will automatically renew certificates with the API server as certificates near expiration.
    audit: |
      Check the kubelet CLI parameters to ensure '--rotate-certificates' is not set to false, and that the kubelet config file has not set 'rotateCertificates' to false.
    remediation: |
      Depending on where the configuration behavior is defined (CLI parameters override config file values), update the kubelet CLI parameters to set '--rotate-certificates' to true, and/or update the kubelet configuration to set 'rotateCertificates' to true.
  query: |
    k8s.kubelet.configuration["rotateCertificates"] != "false"
- uid: mondoo-kubernetes-security-secure-kubelet-config
  title: Ownership and permissions of kubelet configuration should be restricted
  severity: 80
  docs:
    desc: |
      Ensure proper file ownership and read-write-execute permissions for kubelet configuration file.
    audit: |
      View the kubelet configuration file details:

      ```
      $ ls -l /etc/kubernetes/kubelet.conf
      -rw-r--r-- 1 root root 1155 Sep 21 15:03 /etc/kubernetes/kubelet.conf
      ```
    remediation: |
      Update the ownership and permissions:

      ```
      chown root:root /etc/kubernetes/kubelet.conf
      chmod 600 /etc/kubernetes/kubelet.conf
      ```
  query: |
    if (file(k8s.kubelet.configuration['config']).exists) {
      file(k8s.kubelet.configuration['config']) {
        user.name == "root"
        group.name == "root"
      }
      file(k8s.kubelet.configuration['config']).permissions {
        user_readable == true
        user_executable == false
        group_readable == false
        group_writeable == false
        group_executable == false
        other_readable == false
        other_writeable == false
        other_executable == false
      }
    }
- uid: mondoo-kubernetes-security-secure-kubelet-cert-authorities
  title: Specify a kubelet certificate authorities file and ensure proper ownership and permissions
  severity: 100
  docs:
    desc: |
      Ensure appropriate ownership and permissions for the kubelet's certificate authorities configuration file.
    audit: |
      View the ownership and permissions:

      ```
      $ ls -l /etc/srv/kubernetes/pki/ca-certificates.crt
      -rw------- 1 root root 1159 Sep 13 04:14 /etc/srv/kubernetes/pki/ca-certificates.crt
      ```
    remediation: |
      Update the ownership and permissions:

      ```
      chown root:root /etc/srv/kubernetes/pki/ca-certificates.crt
      chmod 600 /etc/srv/kubernetes/pki/ca-certificates.crt
      ```
  query: |
    k8s.kubelet.configuration['authentication']['x509']['clientCAFile'] != null
    if (k8s.kubelet.configuration['authentication']['x509']['clientCAFile'] != null) {
      cafile = k8s.kubelet.configuration["authentication"]["x509"]["clientCAFile"]
      file(cafile) {
        user.name == "root"
        group.name == "root"
      }
      file(cafile).permissions {
        user_readable == true
        user_executable == false
        group_readable == false
        group_writeable == false
        group_executable == false
        other_readable == false
        other_writeable == false
        other_executable == false
      }
    }
- uid: mondoo-kubernetes-security-secure-kube-apiserver-yml
  title: Set secure file permissions on the API server pod specification file
  docs:
    desc: Ensure that the API server pod specification file has permissions of `600` and is owned by `root:root`.
    remediation: |-
      Run this command on the Control Plane node:

      ```
      chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml
      chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml
      ```
  query: |
    if (file("/etc/kubernetes/manifests/kube-apiserver.yaml").exists) {
      file("/etc/kubernetes/manifests/kube-apiserver.yaml") {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == false
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "root"
        group.name == "root"
      }
    }
- uid: mondoo-kubernetes-security-secure-etcd-data-dir
  title: Set secure directory permissions on the etcd data directory
  docs:
    desc: Ensure that the etcd data directory has permissions of `700` and is owned by `etcd:etcd`.
    remediation: |-
      On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:

      ```
      ps -ef | grep etcd
      ```

      Run the below command:

      ```
      chmod 700 /var/lib/etcd
      ```
  query: |
    if (file("/var/lib/etcd").exists) {
      file("/var/lib/etcd") {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == true
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "etcd"
        group.name == "etcd"
      }
    } else {
      dir = processes.where( executable == /etcd/ ).list[0].flags["data-dir"]
      file(dir) {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == true
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "etcd"
        group.name == "etcd"
      }
    }
- uid: mondoo-kubernetes-security-secure-admin-conf
  title: Set secure file permissions on the admin.conf file
  docs:
    desc: Ensure that the `admin.conf` file has permissions of `600` and is owned by root:root.
    remediation: |-
      Run this command on the Control Plane node:

      ```
      chmod 600 /etc/kubernetes/admin.conf
      chown root:root /etc/kubernetes/admin.conf
      ```
  query: |
    if (file("/etc/kubernetes/admin.conf").exists) {
      file("/etc/kubernetes/admin.conf") {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == false
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "root"
        group.name == "root"
      }
    }
- uid: mondoo-kubernetes-security-secure-scheduler_conf
  title: Set secure file permissions on the scheduler.conf file
  docs:
    desc: Ensure that the `scheduler.conf` file has permissions of `600` and is owned by `root:root`.
    remediation: |-
      Run this command on the Control Plane node:

      ```
      chmod 600 /etc/kubernetes/scheduler.conf
      chown root:root /etc/kubernetes/scheduler.conf
      ```
  query: |
    if (file("/etc/kubernetes/scheduler.conf").exists) {
      file("/etc/kubernetes/scheduler.conf") {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == false
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "root"
        group.name == "root"
      }
    }
- uid: mondoo-kubernetes-security-secure-controller-manager_conf
  title: Set secure file permissions on the controller-manager.conf file
  docs:
    desc: Ensure that the `controller-manager.conf` file has permissions of `600` and is owned by `root:root`.
    remediation: |-
      Run this command on the Control Plane node:

      ```
      chmod 600 /etc/kubernetes/controller-manager.conf
      chown root:root /etc/kubernetes/controller-manager.conf
      ```
  query: |
    if (file("/etc/kubernetes/controller-manager.conf").exists) {
      file("/etc/kubernetes/controller-manager.conf") {
        permissions.user_writeable == true
        permissions.group_writeable == false
        permissions.other_writeable == false
        permissions.user_readable == true
        permissions.group_readable == false
        permissions.other_readable == false
        permissions.user_executable == false
        permissions.group_executable == false
        permissions.other_executable == false
        user.name == "root"
        group.name == "root"
      }
    }
- uid: mondoo-kubernetes-security-secure-pki-directory
  title: Ensure that the Kubernetes PKI/SSL directory is owned by root:root
  docs:
    desc: Ensure that the Kubernetes PKI/SSL directory is owned by `root:root`.
    remediation: |-
      Run one of the following commands on the Control Plane node depending on the location of your PKI/SSL directory:

      ```
      chown -R root:root /etc/kubernetes/pki/
      ```
      
      or

      ```
      chown -R root:root /etc/kubernetes/ssl/
      ````
  query: |
    if (file("/etc/kubernetes/ssl/").exists) {
      file("/etc/kubernetes/ssl/") {
        user.name == "root"
        group.name == "root"
      }
    } else {
      file("/etc/kubernetes/pki/") {
        user.name == "root"
        group.name == "root"
      }
    }
- uid: mondoo-kubernetes-security-https-api-server
  title: Ensure the kube-apiserver is not listening on an insecure HTTP port
  docs:
    desc: Ensure the kube-apiserver is not listening on an insecure HTTP port.
    remediation: |-
      Find the kube-apiserver process and check the `insecure-port` argument. If the argument is set to `0`, then the kube-apiserver is not listening on an insecure HTTP port:
      ```
      ps aux | grep kube-apiserver
      ```
  query: |
    processes.where(executable == /kube-apiserver/).list {
      flags["insecure-port"] == 0
    }
- uid: mondoo-kubernetes-security-api-server-no-anonymous-auth
  title: Ensure the kube-apiserver does not allow anonymous authentication
  docs:
    desc: Ensure the kube-apiserver does not allow anonymous authentication.
    remediation: |-
      Find the kube-apiserver process and check the `--anonymous-auth` argument. If the argument is set to `false`, then the kube-apiserver does not allow anonymous authentication:
      ```
      ps aux | grep kube-apiserver
      ```
  query: |
    processes.where(executable == /kube-apiserver/).list {
      flags["anonymous-auth"] == "false"
    }
# Pod queries
- uid: mondoo-kubernetes-security-pod-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.pod {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: mondoo-kubernetes-security-cronjob-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.cronjob {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: mondoo-kubernetes-security-statefulset-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.statefulset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: mondoo-kubernetes-security-deployment-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.deployment {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: mondoo-kubernetes-security-job-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.job {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: mondoo-kubernetes-security-replicaset-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.replicaset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: mondoo-kubernetes-security-daemonset-docker-socket
  title: Container should not mount the Docker socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/docker.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/docker.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/docker.sock
                name: vol
      ```
  query: |
    k8s.daemonset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/docker.sock')
    }
- uid: mondoo-kubernetes-security-pod-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.pod {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: mondoo-kubernetes-security-cronjob-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.cronjob {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: mondoo-kubernetes-security-statefulset-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.statefulset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: mondoo-kubernetes-security-deployment-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.deployment {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: mondoo-kubernetes-security-job-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.job {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: mondoo-kubernetes-security-replicaset-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.replicaset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: mondoo-kubernetes-security-daemonset-containerd-socket
  title: Container should not mount the containerd socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /run/containerd/containerd.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /run/containerd/containerd.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /run/containerd/containerd.sock
                name: vol
      ```
  query: |
    k8s.daemonset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/run/containerd/containerd.sock')
    }
- uid: mondoo-kubernetes-security-pod-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.pod {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: mondoo-kubernetes-security-cronjob-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.cronjob {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: mondoo-kubernetes-security-statefulset-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.statefulset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: mondoo-kubernetes-security-deployment-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.deployment {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: mondoo-kubernetes-security-job-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.job {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: mondoo-kubernetes-security-replicaset-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.replicaset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: mondoo-kubernetes-security-daemonset-crio-socket
  title: Container should not mount the CRI-O socket
  severity: 100
  docs:
    audit: |
      Check for the existence of `hostPath.path: /var/run/crio/crio.sock` setting in the `volumes`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        volumes:
          - name: vol
            hostPath:
              - path: /var/run/crio/crio.sock
        containers:
          - name: app
            image: images.my-company.example/app:v1
            volumeMounts:
              - mountPath: /var/run/crio/crio.sock
                name: vol
      ```
  query: |
    k8s.daemonset {
      podSpec['volumes'] == null || podSpec['volumes'].all(_['hostPath']['path'] != '/var/run/crio/crio.sock')
    }
- uid: mondoo-kubernetes-security-pod-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: mondoo-kubernetes-security-cronjob-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: mondoo-kubernetes-security-statefulset-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: mondoo-kubernetes-security-deployment-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: mondoo-kubernetes-security-job-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: mondoo-kubernetes-security-replicaset-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: mondoo-kubernetes-security-daemonset-allowprivilegeescalation
  title: Container should not allow privilege escalation
  severity: 100
  docs:
    audit: |
      Check for the existence of `allowPrivilegeEscalation: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            securityContext:
              allowPrivilegeEscalation: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
      containers {
        # @msg Container ${ _.name  } should set allowPrivilegeEscalation to false
        securityContext['allowPrivilegeEscalation'] != true
      }
    }
- uid: mondoo-kubernetes-security-pod-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: mondoo-kubernetes-security-cronjob-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: mondoo-kubernetes-security-statefulset-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: mondoo-kubernetes-security-deployment-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means that the container has the host's capabilities including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: mondoo-kubernetes-security-job-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: mondoo-kubernetes-security-replicaset-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: mondoo-kubernetes-security-daemonset-privilegedcontainer
  title: Container should not run as a privileged container
  severity: 100
  docs:
    desc: |
      Running a privileged container means the container has the host's capabilities, including access to all devices and the host's network.
    audit: |
      Check for the existence of `privileged: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: true
      ```
    remediation: |
      Remove the `privileged` setting from the container spec:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
      
      Or explicitly set `privileged` to `false`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              privileged: false
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
      containers {
        # @msg Container ${ _.name  } should not set `privileged` to `true`
        securityContext['privileged'] != true
      }
    }
- uid: mondoo-kubernetes-security-pod-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: mondoo-kubernetes-security-cronjob-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: mondoo-kubernetes-security-statefulset-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: mondoo-kubernetes-security-deployment-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: mondoo-kubernetes-security-job-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: mondoo-kubernetes-security-replicaset-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: mondoo-kubernetes-security-daemonset-readonlyrootfilesystem
  title: Container should use an immutable root filesystem
  severity: 80
  docs:
    desc: |
      Running a container with an immutable (read-only) file system prevents the modification of running containers.
    audit: |
      Check for the existence of `readOnlyRootFilesystem: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              readOnlyRootFilesystem: true
      ```
  refs:
    - title: Configure a Security Context for a Pod or Container
      url: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
      containers {
        # @msg Container ${ _.name  } should set readOnlyRootFilesystem to true
        securityContext['readOnlyRootFilesystem'] == true
      }
    }
- uid: mondoo-kubernetes-security-pod-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.pod {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: mondoo-kubernetes-security-cronjob-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.cronjob {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: mondoo-kubernetes-security-statefulset-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.statefulset {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: mondoo-kubernetes-security-deployment-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.deployment {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: mondoo-kubernetes-security-job-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.job {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: mondoo-kubernetes-security-replicaset-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.replicaset {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: mondoo-kubernetes-security-daemonset-runasnonroot
  title: Container should not run as root
  severity: 100
  docs:
    desc: |
      Set the `runAsNonRoot: true` `securityContext` to ensure containers do not run as the root user.
    audit: |
      Check for the existence of `runAsNonRoot: true` setting in the `securityContext`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
            securityContext:
              runAsNonRoot: true
      ```

      It is also possible to set it for all containers at the Pod level:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        securityContext:
          runAsNonRoot: true
        containers:
          - name: container-name
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.daemonset {
      podSecurityContext=podSpec['securityContext']
      initContainers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
      containers {
        a = podSecurityContext['runAsNonRoot'] == true && securityContext['runAsNonRoot'] == null
        res = securityContext['runAsNonRoot'] == true || a
        # @msg Container ${ _.name  } should set runAsNonRoot to true
        res == true
      }
    }
- uid: mondoo-kubernetes-security-pod-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network including loopback devices. This capability can be used to intercept network traffic including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.pod {
      podSpec['hostNetwork'] != true
    }
- uid: mondoo-kubernetes-security-cronjob-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.cronjob {
      podSpec['hostNetwork'] != true
    }
- uid: mondoo-kubernetes-security-statefulset-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.statefulset {
      podSpec['hostNetwork'] != true
    }
- uid: mondoo-kubernetes-security-deployment-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.deployment {
      podSpec['hostNetwork'] != true
    }
- uid: mondoo-kubernetes-security-job-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.job {
      podSpec['hostNetwork'] != true
    }
- uid: mondoo-kubernetes-security-replicaset-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.replicaset {
      podSpec['hostNetwork'] != true
    }
- uid: mondoo-kubernetes-security-daemonset-hostnetwork
  title: Pod should not run with hostNetwork
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostNetwork` namespace gives containers access to the host's network, including loopback devices. This capability can be used to intercept network traffic, including the traffic of other pods.
    audit: |
      Check for the existence of `hostNetwork: true` setting in `spec`:

      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        hostNetwork: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.daemonset {
      podSpec['hostNetwork'] != true
    }
- uid: mondoo-kubernetes-security-pod-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.pod {
      podSpec['hostPID'] != true
    }
- uid: mondoo-kubernetes-security-cronjob-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.cronjob {
      podSpec['hostPID'] != true
    }
- uid: mondoo-kubernetes-security-statefulset-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.statefulset {
      podSpec['hostPID'] != true
    }
- uid: mondoo-kubernetes-security-deployment-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.deployment {
      podSpec['hostPID'] != true
    }
- uid: mondoo-kubernetes-security-job-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.job {
      podSpec['hostPID'] != true
    }
- uid: mondoo-kubernetes-security-replicaset-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.replicaset {
      podSpec['hostPID'] != true
    }
- uid: mondoo-kubernetes-security-daemonset-hostpid
  title: Pod should not run with hostPID
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |-
      Running pods with the `hostPID` namespace gives containers access to the host's process ID namespace and can be used to escalate privileges outside a container.
    audit: |
      Check for the existence of `hostPID: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostPID: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.daemonset {
      podSpec['hostPID'] != true
    }
- uid: mondoo-kubernetes-security-pod-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.pod {
      podSpec['hostIPC'] != true
    }
- uid: mondoo-kubernetes-security-cronjob-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.cronjob {
      podSpec['hostIPC'] != true
    }
- uid: mondoo-kubernetes-security-statefulset-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.statefulset {
      podSpec['hostIPC'] != true
    }
- uid: mondoo-kubernetes-security-deployment-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.deployment {
      podSpec['hostIPC'] != true
    }
- uid: mondoo-kubernetes-security-job-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.job {
      podSpec['hostIPC'] != true
    }
- uid: mondoo-kubernetes-security-replicaset-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.replicaset {
      podSpec['hostIPC'] != true
    }
- uid: mondoo-kubernetes-security-daemonset-hostipc
  title: Pod should not run with hostIPC
  severity: 80
  refs:
    - title: Host namespaces
      url: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-namespaces
  docs:
    desc: |
      Enabling `hostIPC` gives containers access to the host's IPC namespace and breaks container isolation.
    audit: |
      Check for the existence of `hostIPC: true` setting in `spec`:

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        hostIPC: true
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```
  query: |
    k8s.daemonset {
      podSpec['hostIPC'] != true
    }
- uid: mondoo-kubernetes-security-pod-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.pod {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: mondoo-kubernetes-security-cronjob-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.cronjob {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: mondoo-kubernetes-security-statefulset-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.statefulset {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: mondoo-kubernetes-security-deployment-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.deployment {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: mondoo-kubernetes-security-job-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.job {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: mondoo-kubernetes-security-replicaset-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.replicaset {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: mondoo-kubernetes-security-daemonset-serviceaccount
  title: Pod should not run with the default service account
  severity: 30
  docs:
    desc: |
      Pods that interact with the Kubernetes API using a ServiceAccount should use specific ServiceAccounts with only the permissions necessary instead of the default ServiceAccount that is included in every Namespace (named 'default'). The only valid use for the default ServiceAccount is for Pods that set '.spec.automountServiceAccountToken' to 'false'. In this case, the Pod explicitly asks for no ServiceAccount to be mounted into the Pod's filesystem, and the Pod is therefore a ServiceAccount-less Pod.
    audit: |
      Check that Pods do not set the legacy '.spec.serviceAccount':

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccount: some-account
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Check that Pods do not set the '.spec.serviceAccountName' to the empty string (which is interpreted as 'default'), or to 'default'.

      ```yaml
      apiVersion: v1
      kind: Pod
      spec:
        serviceAccountName: ""
        containers:
          - name: example-app
            image: index.docker.io/yournamespace/repository
      ```

      Even when the deprecated field '.spec.serviceAccount' is not specified, it will get populated by Kubernetes inside the cluster when a manifest is applied.
      Because of that, we also need to check for the field.
    remediation: |
      Create a ServiceAccount specifically for the Pod with only the permissions it needs when interacting with the Kubernetes API. Update the Pod's '.spec.serviceAccountName' to the name of the ServiceAccount created for the Pod.

      Or if the Pod doesn't interact with the Kubernetes API, set the Pod's `.spec.automountServiceAccountToken` field to false so that no ServiceAccount is available to the Pod.
    refs:
      - title: Configure Service Accounts for Pods
        url: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  query: |
    k8s.daemonset {
      podSpec['serviceAccount'] == null || podSpec['serviceAccount'] == podSpec['serviceAccountName']
      podSpec['serviceAccountName'] != '' || podSpec['automountServiceAccountToken'] == false
      podSpec['serviceAccountName'] != 'default' || podSpec['automountServiceAccountToken'] == false
    }
- uid: mondoo-kubernetes-security-pod-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: mondoo-kubernetes-security-cronjob-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: mondoo-kubernetes-security-statefulset-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: mondoo-kubernetes-security-deployment-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: mondoo-kubernetes-security-job-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: mondoo-kubernetes-security-replicaset-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: mondoo-kubernetes-security-daemonset-imagepull
  title: Container image pull should be consistent
  severity: 60
  docs:
    desc: |
      It's important that each time a pod is started the same container is pulled, so that services across pods behave the same. To ensure the same container is always used, manifests should set `imagePullPolicy: Always` and the `image` configuration should pull either a tag or a digest (SHA). 
    audit: |
      Check for the existence of `imagePullPolicy: Always` and ensure `image` uses either a tag or a digest (SHA):
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            imagePullPolicy: Always
      ```
  # - image needs to be <image-name>:<tag> or <image-name>@<digest>
  # - avoid :latest tag
  # - use imagePullPolicy: Always since the underlying system caches it anyway
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
      containers {
        # @msg Container ${ _.name  } should set imagePullPolicy to Always
        imagePullPolicy == 'Always'

        correctImage = image != /:latest/ && image.contains(':') == true

        # @msg Container ${ _.name  } should set an image tag or digest
        correctImage == true
      }
    }
- uid: mondoo-kubernetes-security-pod-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: mondoo-kubernetes-security-cronjob-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: mondoo-kubernetes-security-statefulset-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: mondoo-kubernetes-security-deployment-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: mondoo-kubernetes-security-job-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: mondoo-kubernetes-security-replicaset-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: mondoo-kubernetes-security-daemonset-limitcpu
  title: Container should have a CPU limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set CPU limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of CPU resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
    remediation: |
      Define the required resources for CPU `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                cpu: "500m"
      ```
  refs:
    - title: Resource Management for Pods and Containers
      url: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: mondoo-kubernetes-security-pod-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.pod {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: mondoo-kubernetes-security-cronjob-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.cronjob {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: mondoo-kubernetes-security-statefulset-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.statefulset {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: mondoo-kubernetes-security-deployment-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.deployment {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set CPU limits
        resources['limits']['cpu'] != null
      }
    }
- uid: mondoo-kubernetes-security-job-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.job {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: mondoo-kubernetes-security-replicaset-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.replicaset {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: mondoo-kubernetes-security-daemonset-limitmemory
  title: Container should have a memory limit
  severity: 20
  docs:
    desc: |
      Kubernetes pod configurations should set memory limits for containers defined in the manifest. This prevents the pod from exhausting the host's resources in case of an application malfunction or an attack.
    audit: |
      Check for the existence of memory resources in `limits`:
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
    remediation: |
      Define the required resources for memory `limits` in the manifest: 
      
      ```yaml
      ---
      apiVersion: v1
      kind: Pod
      spec:
        containers:
          - name: app
            image: images.my-company.example/app:v1
            resources:
              limits:
                memory: "1Gi"
      ```
  query: |
    k8s.daemonset {
      initContainers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
      containers {
        # @msg Container ${ _.name  } should set memory limits
        resources['limits']['memory'] != null
      }
    }
- uid: mondoo-kubernetes-security-pod-capability-net-raw
  title: Pods should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      Pods should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no Pods have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


      Additionally, a Pod that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these Pods with:

      ```kubectl get pods -A -o json | jq -r '.items[] | select( .spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any Pods that explicitly add the NET_RAW or ALL capability, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: examplePod
        namespace: example-namespace
      spec:
        containers:
          - securityContext:
              capabilities:
                add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any Pods that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: example
        namespace: example-namespace
      spec:
        containers:
          - securityContext:
              capabilities:
                drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    k8s.pod {
      podSpec['containers'] {
        _['securityContext']['capabilities'] != null
        _['securityContext']['capabilities'] {
          _['add'] == null || _['add'].none(_.upcase == "ALL")
          _['add'] == null || _['add'].none(_.upcase == "NET_RAW")
          _['drop'] != null
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: mondoo-kubernetes-security-daemonset-capability-net-raw
  title: DaemonSets should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      DaemonSets should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no DaemonSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get daemonsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


      Additionally, a DaemonSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

      ```kubectl get daemonsets -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any DaemonSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any DaemonSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    k8s.daemonset {
      podSpec['containers'] {
        _['securityContext']['capabilities'] != null
        _['securityContext']['capabilities'] {
          _['add'] == null || _['add'].none(_.upcase == "ALL")
          _['add'] == null || _['add'].none(_.upcase == "NET_RAW")
          _['drop'] != null
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: mondoo-kubernetes-security-replicaset-capability-net-raw
  title: ReplicaSets should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      ReplicaSets should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no ReplicaSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get replicasets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


      Additionally, a ReplicaSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

      ```kubectl get replicasets -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any ReplicaSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: ReplicaSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any ReplicaSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: apps/v1
      kind: ReplicaSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    k8s.replicaset {
      podSpec['containers'] {
        _['securityContext']['capabilities'] != null
        _['securityContext']['capabilities'] {
          _['add'] == null || _['add'].none(_.upcase == "ALL")
          _['add'] == null || _['add'].none(_.upcase == "NET_RAW")
          _['drop'] != null
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: mondoo-kubernetes-security-job-capability-net-raw
  title: Jobs should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      Jobs should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no Jobs have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get jobs -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


      Additionally, a Job that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

      ```kubectl get jobs -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any Jobs that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any Jobs that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    k8s.job {
      podSpec['containers'] {
        _['securityContext']['capabilities'] != null
        _['securityContext']['capabilities'] {
          _['add'] == null || _['add'].none(_.upcase == "ALL")
          _['add'] == null || _['add'].none(_.upcase == "NET_RAW")
          _['drop'] != null
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: mondoo-kubernetes-security-deployment-capability-net-raw
  title: Deployments should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      Deployments should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no Deployments have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get deployments -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


      Additionally, a Deployment that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

      ```kubectl get deployments -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any Deployments that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any Deployments that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    k8s.deployment {
      podSpec['containers'] {
        _['securityContext']['capabilities'] != null
        _['securityContext']['capabilities'] {
          _['add'] == null || _['add'].none(_.upcase == "ALL")
          _['add'] == null || _['add'].none(_.upcase == "NET_RAW")
          _['drop'] != null
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: mondoo-kubernetes-security-statefulset-capability-net-raw
  title: StatefulSets should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      StatefulSets should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no StatefulSets have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get statefulsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


      Additionally, a StatefulSet that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

      ```kubectl get statefulsets -A -o json | jq -r '.items[] | select( .spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any StatefulSets that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any StatefulSets that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    k8s.statefulset {
      podSpec['containers'] {
        _['securityContext']['capabilities'] != null
        _['securityContext']['capabilities'] {
          _['add'] == null || _['add'].none(_.upcase == "ALL")
          _['add'] == null || _['add'].none(_.upcase == "NET_RAW")
          _['drop'] != null
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: mondoo-kubernetes-security-cronjob-capability-net-raw
  title: CronJobs should not run with NET_RAW capability
  severity: 80
  docs:
    desc: |
      CronJobs should not run with NET_RAW capability. This allows a process to write raw packets to the network interface which can allow crafting packets like malicious ARP and/or DNS responses.
    audit: |
      Check to ensure no CronJobs have explicitly asked for the NET_RAW capability (or asked for ALL capabilities which includes NET_RAW):

      ```kubectl get cronjobs -A -o json | jq -r '.items[] | select(.spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|NET_RAW")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```


      Additionally, a CronJob that doesn't define a list of capabilities to drop at all, or that has a non-empty drop list that doesn't drop NET_RAW (or the ALL capability which includes NET_RAW) will implicitly run with NET_RAW. List these DaemonSets with:

      ```kubectl get cronjobs -A -o json | jq -r '.items[] | select( .spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.drop | . == null or ( any(.[] ; ascii_upcase | test("ALL|NET_RAW")) | not) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any CronJobs that explicitly add the NET_RAW or ALL capability, update them to ensure they do not ask for the NET_RAW or ALL capability:

      ```yaml
      apiVersion: batch/v1
      kind: CronJob
      metadata:
        name: example
        namespace: example-namespace
      spec:
        jobTemplate:
          spec:
            template:
              spec:
                containers:
                  - securityContext:
                      capabilities:
                        add: [] # <-- ensure no "NET_RAW" or "ALL" in the list of capabilities added
      ```

      For any CronJobs that do not define a list of capabilities to drop or that define a list but do not drop NET_RAW, update them to ensure they drop ALL or NET_RAW:

      ```yaml
      apiVersion: batch/v1
      kind: CronJob
      metadata:
        name: example
        namespace: example-namespace
      spec:
        jobTemplate:
          spec:
            template:
              spec:
                containers:
                  - securityContext:
                      capabilities:
                        drop: ["NET_RAW"] # <-- or ensure "ALL" in the list of capabilities to drop
      ```
  query: |
    k8s.cronjob {
      podSpec['containers'] {
        _['securityContext']['capabilities'] != null
        _['securityContext']['capabilities'] {
          _['add'] == null || _['add'].none(_.upcase == "ALL")
          _['add'] == null || _['add'].none(_.upcase == "NET_RAW")
          _['drop'] != null
          _['drop'].any(_.upcase == "NET_RAW") || _['drop'].any(_.upcase == "ALL")
        }
      }
    }
- uid: mondoo-kubernetes-security-pod-capability-sys-admin
  title: Pods should not run with SYS_ADMIN capability
  severity: 80
  docs:
    desc: |
      Pods should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
    audit: |
      Check to ensure no Pods have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

      ```kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any Pods that explicitly add the SYS_ADMIN or ALL capability, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not ask for the SYS_ADMIN or ALL capability:

      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: examplePod
        namespace: example-namespace
      spec:
        containers:
          - securityContext:
              capabilities:
                add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
      ```
  query: |
    k8s.pod {
      podSpec['containers'] {
        if( _['securityContext']['capabilities'] != null ) {
          _['securityContext']['capabilities'] {
            _['add'] == null || _['add'].none(_.upcase == "ALL")
            _['add'] == null || _['add'].none(_.upcase == "SYS_ADMIN")
          }
        } else {
            true
        }
      }
    }
- uid: mondoo-kubernetes-security-daemonset-capability-sys-admin
  title: DaemonSets should not run with SYS_ADMIN capability
  severity: 80
  docs:
    desc: |
      DaemonSets should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
    audit: |
      Check to ensure no DaemonSets have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

      ```kubectl get daemonsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any DaemonSets that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
      ```
  query: |
    k8s.daemonset {
      podSpec['containers'] {
        if( _['securityContext']['capabilities'] != null ) {
          _['securityContext']['capabilities'] {
            _['add'] == null || _['add'].none(_.upcase == "ALL")
            _['add'] == null || _['add'].none(_.upcase == "SYS_ADMIN")
          }
        } else {
            true
        }
      }
    }
- uid: mondoo-kubernetes-security-replicaset-capability-sys-admin
  title: ReplicaSets should not run with SYS_ADMIN capability
  severity: 80
  docs:
    desc: |
      ReplicaSets should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
    audit: |
      Check to ensure no ReplicaSets have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

      ```kubectl get replicasets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any ReplicaSets that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: ReplicaSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
      ```
  query: |
    k8s.replicaset {
      podSpec['containers'] {
        if( _['securityContext']['capabilities'] != null ) {
          _['securityContext']['capabilities'] {
            _['add'] == null || _['add'].none(_.upcase == "ALL")
            _['add'] == null || _['add'].none(_.upcase == "SYS_ADMIN")
          }
        } else {
            true
        }
      }
    }
- uid: mondoo-kubernetes-security-job-capability-sys-admin
  title: Jobs should not run with SYS_ADMIN capability
  severity: 80
  docs:
    desc: |
      Jobs should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
    audit: |
      Check to ensure no Jobs have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

      ```kubectl get jobs -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any Jobs that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

      ```yaml
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
      ```
  query: |
    k8s.job {
      podSpec['containers'] {
        if( _['securityContext']['capabilities'] != null ) {
          _['securityContext']['capabilities'] {
            _['add'] == null || _['add'].none(_.upcase == "ALL")
            _['add'] == null || _['add'].none(_.upcase == "SYS_ADMIN")
          }
        } else {
            true
        }
      }
    }
- uid: mondoo-kubernetes-security-deployment-capability-sys-admin
  title: Deployments should not run with SYS_ADMIN capability
  severity: 80
  docs:
    desc: |
      Deployments should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
    audit: |
      Check to ensure no Deployments have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

      ```kubectl get deployments -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any Deployments that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
      ```
  query: |
    k8s.deployment {
      podSpec['containers'] {
        if( _['securityContext']['capabilities'] != null ) {
          _['securityContext']['capabilities'] {
            _['add'] == null || _['add'].none(_.upcase == "ALL")
            _['add'] == null || _['add'].none(_.upcase == "SYS_ADMIN")
          }
        } else {
            true
        }
      }
    }
- uid: mondoo-kubernetes-security-statefulset-capability-sys-admin
  title: StatefulSets should not run with SYS_ADMIN capability
  severity: 80
  docs:
    desc: |
      StatefulSets should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
    audit: |
      Check to ensure no StatefulSets have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

      ```kubectl get statefulsets -A -o json | jq -r '.items[] | select(.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any StatefulSets that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

      ```yaml
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - securityContext:
                  capabilities:
                    add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
      ```
  query: |
    k8s.statefulset {
      podSpec['containers'] {
        if( _['securityContext']['capabilities'] != null ) {
          _['securityContext']['capabilities'] {
            _['add'] == null || _['add'].none(_.upcase == "ALL")
            _['add'] == null || _['add'].none(_.upcase == "SYS_ADMIN")
          }
        } else {
            true
        }
      }
    }
- uid: mondoo-kubernetes-security-cronjob-capability-sys-admin
  title: CronJobs should not run with SYS_ADMIN capability
  severity: 80
  docs:
    desc: |
      CronJobs should not run with SYS_ADMIN capability. The SYS_ADMIN capability enables a wide range of elevated system calls.
    audit: |
      Check to ensure no CronJobs have explicitly asked for the SYS_ADMIN capability (or asked for ALL capabilities which includes SYS_ADMIN):

      ```kubectl get cronjobs -A -o json | jq -r '.items[] | select(.spec.jobTemplate.spec.template.spec.containers[].securityContext.capabilities.add | . != null and any(.[] ; ascii_upcase | test("ALL|SYS_ADMIN")) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any CronJobs that explicitly add the SYS_ADMIN or ALL capability, update them to ensure they do not ask for the SYS_ADMIN or ALL capability:

      ```yaml
      apiVersion: batch/v1
      kind: CronJob
      metadata:
        name: example
        namespace: example-namespace
      spec:
        jobTemplate:
          spec:
            template:
              spec:
                containers:
                  - securityContext:
                      capabilities:
                        add: [] # <-- ensure no "SYS_ADMIN" or "ALL" in the list of capabilities added
      ```
  query: |
    k8s.cronjob {
      podSpec['containers'] {
        if( _['securityContext']['capabilities'] != null ) {
          _['securityContext']['capabilities'] {
            _['add'] == null || _['add'].none(_.upcase == "ALL")
            _['add'] == null || _['add'].none(_.upcase == "SYS_ADMIN")
          }
        } else {
            true
        }
      }
    }
- uid: mondoo-kubernetes-security-pod-ports-hostport
  title: Pods should not bind to a host port
  severity: 80
  docs:
    desc: |
      Pods should not bind to the underlying host port. This allows bypassing certain network access control systems.
    audit: |
      Check to ensure no Pods are binding any of their containers to a host port:

      ```kubectl get pods -A -o json | jq -r '.items[] | select( (.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
    remediation: |
      For any Pods that bind to a host port, update the Pods (or the Deployments/DaemonSets/CronJobs/etc that produced the Pods) to ensure they do not bind to a host port:

      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: example
        namespace: example-namespace
      spec:
        containers:
          - ports:
            - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
              name: http
              protocol: TCP
            - containerPort: 443
              name: https
              protocol: TCP
      ```
  query: |
    k8s.pod.podSpec {
      _['containers'] {
        _['name']
        _['ports'] == null || _['ports'].all(_['hostPort'] == null)
      }
    }
- uid: mondoo-kubernetes-security-daemonset-ports-hostport
  title: DaemonSets should not bind to a host port
  severity: 80
  docs:
    desc: |
      DaemonSets should not bind to the underlying host port. This allows bypassing certain network access control systems.
    audit: |
      Check to ensure no DaemonSets are binding any of their containers to a host port:

      ```kubectl get daemonsets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
    remediation: |
      For any DaemonSets that bind to a host port, update the DaemonSets to ensure they do not bind to a host port:

      ```yaml
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - ports:
                - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                  name: http
                  protocol: TCP
                - containerPort: 443
                  name: https
                  protocol: TCP
      ```
  query: |
    k8s.daemonset.podSpec {
      _['containers'] {
        _['name']
        _['ports'] == null || _['ports'].all(_['hostPort'] == null)
      }
    }
- uid: mondoo-kubernetes-security-replicaset-ports-hostport
  title: ReplicaSets should not bind to a host port
  severity: 80
  docs:
    desc: |
      ReplicaSets should not bind to the underlying host port. This allows bypassing certain network access control systems.
    audit: |
      Check to ensure no ReplicaSets are binding any of their containers to a host port:

      ```kubectl get replicasets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
    remediation: |
      For any ReplicaSets that bind to a host port, update the ReplicaSets to ensure they do not bind to a host port:

      ```yaml
      apiVersion: apps/v1
      kind: ReplicaSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - ports:
                - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                  name: http
                  protocol: TCP
                - containerPort: 443
                  name: https
                  protocol: TCP
      ```
  query: |
    k8s.replicaset.podSpec {
      _['containers'] {
        _['name']
        _['ports'] == null || _['ports'].all(_['hostPort'] == null)
      }
    }
- uid: mondoo-kubernetes-security-job-ports-hostport
  title: Jobs should not bind to a host port
  severity: 80
  docs:
    desc: |
      Jobs should not bind to the underlying host port. This allows bypassing certain network access control systems.
    audit: |
      Check to ensure no Jobs are binding any of their containers to a host port:

      ```kubectl get jobs -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
    remediation: |
      For any ReplicaSets that bind to a host port, update the Jobs to ensure they do not bind to a host port:

      ```yaml
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - ports:
                - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                  name: http
                  protocol: TCP
                - containerPort: 443
                  name: https
                  protocol: TCP
      ```
  query: |
    k8s.job.podSpec {
      _['containers'] {
        _['name']
        _['ports'] == null || _['ports'].all(_['hostPort'] == null)
      }
    }
- uid: mondoo-kubernetes-security-deployment-ports-hostport
  title: Deployments should not bind to a host port
  severity: 80
  docs:
    desc: |
      Deployments should not bind to the underlying host port. This allows bypassing certain network access control systems.
    audit: |
      Check to ensure no Deployments are binding any of their containers to a host port:

      ```kubectl get deployments -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
    remediation: |
      For any Deployments that bind to a host port, update the Deployments to ensure they do not bind to a host port:

      ```yaml
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - ports:
                - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                  name: http
                  protocol: TCP
                - containerPort: 443
                  name: https
                  protocol: TCP
      ```
  query: |
    k8s.deployment.podSpec {
      _['containers'] {
        _['name']
        _['ports'] == null || _['ports'].all(_['hostPort'] == null)
      }
    }
- uid: mondoo-kubernetes-security-statefulset-ports-hostport
  title: StatefulSets should not bind to a host port
  severity: 80
  docs:
    desc: |
      StatefulSets should not bind to the underlying host port. This allows bypassing certain network access control systems.
    audit: |
      Check to ensure no StatefulSets are binding any of their containers to a host port:

      ```kubectl get statefulsets -A -o json | jq -r '.items[] | select( (.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
    remediation: |
      For any StatefulSets that bind to a host port, update the StatefulSets to ensure they do not bind to a host port:

      ```yaml
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - ports:
                - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                  name: http
                  protocol: TCP
                - containerPort: 443
                  name: https
                  protocol: TCP
      ```
  query: |
    k8s.statefulset.podSpec {
      _['containers'] {
        _['name']
        _['ports'] == null || _['ports'].all(_['hostPort'] == null)
      }
    }
- uid: mondoo-kubernetes-security-cronjob-ports-hostport
  title: CronJobs should not bind to a host port
  severity: 80
  docs:
    desc: |
      CronJobs should not bind to the underlying host port. This allows bypassing certain network access control systems.
    audit: |
      Check to ensure no CronJobs are binding any of their containers to a host port:

      ```kubectl get cronjobs -A -o json | jq -r '.items[] | select( (.spec.jobTemplate.spec.template.spec.containers[].ports | . != null and any(.[].hostPort; . != null)  ) ) | .metadata.namespace + "/" + .metadata.name'  | uniq```
    remediation: |
      For any CronJobs that bind to a host port, update the CronJobs to ensure they do not bind to a host port:

      ```yaml
      apiVersion: batch/v1
      kind: CronJob
      metadata:
        name: example
        namespace: example-namespace
      spec:
        jobTemplate:
          spec:
            template:
              spec:
                containers:
                  - ports:
                    - containerPort: 80 # <-- ensure no 'hostPort' is defined in any entries of the port configurations
                      name: http
                      protocol: TCP
                    - containerPort: 443
                      name: https
                      protocol: TCP
      ```
  query: |
    k8s.cronjob.podSpec {
      _['containers'] {
        _['name']
        _['ports'] == null || _['ports'].all(_['hostPort'] == null)
      }
    }
- uid: mondoo-kubernetes-security-pod-hostpath-readonly
  title: Pods should mount any host path volumes as read-only
  severity: 80
  docs:
    desc: |
      Pods should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
    audit: |
      Check to ensure no containers in a Pod are mounting hostPath volumes as read-write:

      ```kubectl get pods -A -o json | jq -r '.items[] | [.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any Pod containers that mount a hostPath volume as read-write, update them (or the Deployment/StatefulSet/etc that created the Pod):

      ```yaml
      apiVersion: v1
      kind: Pod
      metadata:
        name: example
        namespace: example-namespace
      spec:
        containers:
          - volumeMounts:
            - mountPath: /host
              name: hostpath-volume
              readOnly: true # <-- ensure readOnly is set to true
        volumes:
          - hostPath:
              path: /etc
            name: hostpath-volume
      ```
  query: |
    k8s.pod.podSpec {
      hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
      _['containers'] {
        _['name']
        if( _['volumeMounts'] != null ) {
          _['volumeMounts'] {
            n = _['name']
            if( hostPathVolumes.contains(n) ) {
              _['readOnly'] == true
            } else {
              true
            }
          }
        } else {
          true
        }
      }
    }
- uid: mondoo-kubernetes-security-daemonset-hostpath-readonly
  title: DaemonSets should mount any host path volumes as read-only
  severity: 80
  docs:
    desc: |
      DaemonSets should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
    audit: |
      Check to ensure no containers in a DaemonSet are mounting hostPath volumes as read-write:

      ```kubectl get daemonsets -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any DaemonSet containers that mount a hostPath volume as read-write, update them:

      ```yaml
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - volumeMounts:
                - mountPath: /host
                  name: hostpath-volume
                  readOnly: true # <-- ensure readOnly is set to true
            volumes:
              - hostPath:
                  path: /etc
                name: hostpath-volume
      ```
  query: |
    k8s.daemonset.podSpec {
      hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
      _['containers'] {
        _['name']
        if( _['volumeMounts'] != null ) {
          _['volumeMounts'] {
            n = _['name']
            if( hostPathVolumes.contains(n) ) {
              _['readOnly'] == true
            } else {
              true
            }
          }
        } else {
          true
        }
      }
    }
- uid: mondoo-kubernetes-security-replicaset-hostpath-readonly
  title: ReplicaSets should mount any host path volumes as read-only
  severity: 80
  docs:
    desc: |
      ReplicaSets should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
    audit: |
      Check to ensure no containers in a ReplicaSet are mounting hostPath volumes as read-write:

      ```kubectl get replicasets -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any ReplicaSet containers that mount a hostPath volume as read-write, update them:

      ```yaml
      apiVersion: apps/v1
      kind: ReplicaSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - volumeMounts:
                - mountPath: /host
                  name: hostpath-volume
                  readOnly: true # <-- ensure readOnly is set to true
            volumes:
              - hostPath:
                  path: /etc
                name: hostpath-volume
      ```
  query: |
    k8s.replicaset.podSpec {
      hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
      _['containers'] {
        _['name']
        if( _['volumeMounts'] != null ) {
          _['volumeMounts'] {
            n = _['name']
            if( hostPathVolumes.contains(n) ) {
              _['readOnly'] == true
            } else {
              true
            }
          }
        } else {
          true
        }
      }
    }
- uid: mondoo-kubernetes-security-job-hostpath-readonly
  title: Jobs should mount any host path volumes as read-only
  severity: 80
  docs:
    desc: |
      Jobs should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
    audit: |
      Check to ensure no containers in a Job are mounting hostPath volumes as read-write:

      ```kubectl get jobs -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any Job containers that mount a hostPath volume as read-write, update them:

      ```yaml
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - volumeMounts:
                - mountPath: /host
                  name: hostpath-volume
                  readOnly: true # <-- ensure readOnly is set to true
            volumes:
              - hostPath:
                  path: /etc
                name: hostpath-volume
      ```
  query: |
    k8s.job.podSpec {
      hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
      _['containers'] {
        _['name']
        if( _['volumeMounts'] != null ) {
          _['volumeMounts'] {
            n = _['name']
            if( hostPathVolumes.contains(n) ) {
              _['readOnly'] == true
            } else {
              true
            }
          }
        } else {
          true
        }
      }
    }
- uid: mondoo-kubernetes-security-deployment-hostpath-readonly
  title: Deployments should mount any host path volumes as read-only
  severity: 80
  docs:
    desc: |
      Deployments should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
    audit: |
      Check to ensure no containers in a Deployment are mounting hostPath volumes as read-write:

      ```kubectl get deployments -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any Deployment containers that mount a hostPath volume as read-write, update them:

      ```yaml
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - volumeMounts:
                - mountPath: /host
                  name: hostpath-volume
                  readOnly: true # <-- ensure readOnly is set to true
            volumes:
              - hostPath:
                  path: /etc
                name: hostpath-volume
      ```
  query: |
    k8s.deployment.podSpec {
      hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
      _['containers'] {
        _['name']
        if( _['volumeMounts'] != null ) {
          _['volumeMounts'] {
            n = _['name']
            if( hostPathVolumes.contains(n) ) {
              _['readOnly'] == true
            } else {
              true
            }
          }
        } else {
          true
        }
      }
    }
- uid: mondoo-kubernetes-security-statefulset-hostpath-readonly
  title: StatefulSets should mount any host path volumes as read-only
  severity: 80
  docs:
    desc: |
      StatefulSets should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
    audit: |
      Check to ensure no containers in a StatefulSet are mounting hostPath volumes as read-write:

      ```kubectl get statefulsets -A -o json | jq -r '.items[] | [.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any StatefulSet containers that mount a hostPath volume as read-write, update them:

      ```yaml
      apiVersion: apps/v1
      kind: StatefulSet
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - volumeMounts:
                - mountPath: /host
                  name: hostpath-volume
                  readOnly: true # <-- ensure readOnly is set to true
            volumes:
              - hostPath:
                  path: /etc
                name: hostpath-volume
      ```
  query: |
    k8s.statefulset.podSpec {
      hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
      _['containers'] {
        _['name']
        if( _['volumeMounts'] != null ) {
          _['volumeMounts'] {
            n = _['name']
            if( hostPathVolumes.contains(n) ) {
              _['readOnly'] == true
            } else {
              true
            }
          }
        } else {
          true
        }
      }
    }
- uid: mondoo-kubernetes-security-cronjob-hostpath-readonly
  title: CronJobs should mount any host path volumes as read-only
  severity: 80
  docs:
    desc: |
      CronJobs should not mount volumes of type hostPath as read-write. Containers should not be granted the ability to mutate the underlying host they are running on.
    audit: |
      Check to ensure no containers in a CronJob are mounting hostPath volumes as read-write:

      ```kubectl get cronjobs -A -o json | jq -r '.items[] | [.spec.jobTemplate.spec.template.spec.volumes[] | select(.hostPath != null) | .name] as $myVar | select(.spec.jobTemplate.spec.template.spec.containers[].volumeMounts | (. != null and ( .[] | ( [.name] | inside($myVar) ) and .readOnly != true   )  ) ) | .metadata.namespace + "/" + .metadata.name' | uniq```
    remediation: |
      For any CronJob containers that mount a hostPath volume as read-write, update them:

      ```yaml
      apiVersion: batch/v1
      kind: CronJob
      metadata:
        name: example
        namespace: example-namespace
      spec:
        template:
          spec:
            containers:
              - volumeMounts:
                - mountPath: /host
                  name: hostpath-volume
                  readOnly: true # <-- ensure readOnly is set to true
            volumes:
              - hostPath:
                  path: /etc
                name: hostpath-volume
      ```
  query: |
    k8s.cronjob.podSpec {
      hostPathVolumes = _['volumes'].where(_['hostPath'] != null).map(_['name'])
      _['containers'] {
        _['name']
        if( _['volumeMounts'] != null ) {
          _['volumeMounts'] {
            n = _['name']
            if( hostPathVolumes.contains(n) ) {
              _['readOnly'] == true
            } else {
              true
            }
          }
        } else {
          true
        }
      }
    }
- uid: mondoo-kubernetes-security-deployment-tiller
  title: Deployments should not run Tiller (Helm v2)
  severity: 40
  docs:
    desc: |
      Tiller is the in-cluster component for the Helm v2 package manager. It is communicating directly to the Kubernetes API and therefore it has broad RBAC permissions. An attacker can use that to get cluster-wide access.
    audit: |
      Verify there are no deployments running Tiller:
      ```kubectl get deployments -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"```
    remediation: |
      Delete any deployments that are running Tiller.
  query: |
    k8s.deployment.podSpec["containers"].none( _["image"].contains("tiller") )
- uid: mondoo-kubernetes-security-pod-tiller
  title: Pods should not run Tiller (Helm v2)
  severity: 40
  docs:
    desc: |
      Tiller is the in-cluster component for the Helm v2 package manager. It is communicating directly to the Kubernetes API and therefore it has broad RBAC permissions. An attacker can use that to get cluster-wide access.
    audit: |
      Verify there are no pods running Tiller:
      ```kubectl get pods -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"```
    remediation: |
      Delete any pods that are running Tiller.
  query: |
    k8s.pod.podSpec["containers"].none( _["image"].contains("tiller") )
- uid: mondoo-kubernetes-security-deployment-k8s-dashboard
  title: Pods should not run Kubernetes dashboard
  severity: 40
  docs:
    desc: |
      The Kubernetes dashboard allows browsing through cluster resources such as workloads, configmaps and secrets. In 2019 Tesla was hacked because their Kubernetes dashboard was publicly exposed. This allowed the attackers to extract credentials and deploy Bitcoin miners on the cluster.
    audit: |
      Verify there are no deployments running Kubernetes dashboard:
      ```kubectl get deployments -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"```
    remediation: |
      Delete any deployments that are running Kubernetes dashboard.
  query: |
    k8s.deployment.podSpec["containers"].none( _["image"].contains("kubernetes-dashboard") || _["image"].contains("kubernetesui") )
    k8s.deployment.labels["app"] == null || k8s.pod.labels["app"] != "kubernetes-dashboard"
    k8s.deployment.labels["k8s-app"] == null || k8s.pod.labels["k8s-app"] != "kubernetes-dashboard"
- uid: mondoo-kubernetes-security-pod-k8s-dashboard
  title: Pods should not run Kubernetes dashboard
  severity: 40
  docs:
    desc: |
      The Kubernetes dashboard allows browsing through cluster resources such as workloads, configmaps and secrets. In 2019 Tesla was hacked because their Kubernetes dashboard was publicly exposed. This allowed the attackers to extract credentials and deploy Bitcoin miners on the cluster.
    audit: |
      Verify there are no pods running Kubernetes dashboard:
      ```kubectl get pods -A -o=custom-columns="NAME:.metadata.name,IMAGE:.spec.template.spec.containers[*].image"```
    remediation: |
      Delete any pods that are running Kubernetes dashboard.
  query: |
    k8s.pod.podSpec["containers"].none( _["image"].contains("kubernetes-dashboard") || _["image"].contains("kubernetesui") )
    k8s.pod.labels["app"] == null || k8s.pod.labels["app"] != "kubernetes-dashboard"
    k8s.pod.labels["k8s-app"] == null || k8s.pod.labels["k8s-app"] != "kubernetes-dashboard"
# Data Queries
- uid: mondoo-kubernetes-security-gather-deployment-container
  title: Gather all Deployments
  query: |
    k8s.deployments {
      name
    }
- uid: mondoo-kubernetes-security-gather-daemonset-container
  title: Gather all DaemonSets
  query: |
    k8s.daemonsets {
      name
    }
- uid: mondoo-kubernetes-security-gather-statefulset-container
  title: Gather all StatefulSets
  query: |
    k8s.statefulsets {
      name
    }
- uid: mondoo-kubernetes-security-gather-job-container
  title: Gather all Jobs
  query: |
    k8s.jobs {
      name
    }
- uid: mondoo-kubernetes-security-gather-cronjob-container
  title: Gather all CronJobs
  query: |
    k8s.cronjobs {
      name
    }
- uid: mondoo-kubernetes-security-gather-pods-security-context
  title: Gather all Pods with securityContext
  query: |
    k8s.pods {
      name
      namespace
      initContainers {
        name
        resources
        securityContext
      }
      containers {
        name
        resources
        livenessProbe
        securityContext
      }
    }
